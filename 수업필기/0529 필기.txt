<의사결정트리 복습>
트리구조의 혼잡도 계산
-지니계수(default)
: 가장 순수도가 높아질 것인가 를 기준으로 각 컬럼을 분기한다.
-엔트로피
: 혼잡도를 계산하는 공식이 다름

Random Forest
: feature선택을 랜덤하게 한다.
 한 나무에서의 과적합문제가 해소될 수 있다.

* feature importance를 찾아줌 : 설명력이 있음!

<서포트벡터머신 복습>
분류: 공간에서의 데이터를 나누는 경계선을 찾는것
가장 가까이에있는 두 점을 지나는 평행한 두직선의 간격(마진)을 
가장 크게해주는 직선을 찾자. 즉 기울기와 절편을 찾자

선형분류가 아니라 곡선형으로된 분류일 경우, 
고차원으로 바꿔서 feature를 높여서 계산한다.

차원을 높이면 모든 점을 분리할 수 있다.(kernel trick)

주로 쓰는 파라미터는 
c: 이상치의 허용범위를 결정
gamma : 결정경계의 곡률을 결정 
 하나의 데이터가 어느정도까지 영향을 미치는가(클수록 영향이 좁다) (좁아질수록 예민한 모델)
-> c와 gamma를 조절함으로써 혼잡도를 최소화한다.

grid search:

<K Means Clustering>
: 유사한것끼리 grouping, 
1. 설명력이 없음(전문가가 판단하고 활용)
2. 분류분석(classification)을 하기 전에 라벨이 없는 데이터에 라벨을 붙일때
3. 지도학습을 하기 전에 보조적으로 활용, 예측분석

<클러스터링>
모든 데이터는 실루엣점수를 갖는다.
모든점들의 실루엣점수의 평균을 구해서 실루엣지수
데이터의 특징을 찾을 때 사용한다.

agglomerative:
가장 가까운 데이터끼리 하나의 그룹으로 묶어줌
그 다음단계에서 가장 인접한 그룹끼리 묶어줌

DBSCAN: 밀도

kmeans ++: 초기값을 확실하게 주는 것


<거리중심 두 알고리즘의 차이>
병합군집(agglomerative):
각 데이터를 하나의 클러스터로 지정하고, 비슷한 두 클러스터를 합쳐나간다.
군집의 개수가 입력된 값이 되었을 때 종료한다.

K평균 군집(KMeans) : 중심을 K개 설정하고, 각 데이터를 가장 가까운 클러스터 중심에 할당한다.
데이터 포인트에 변화가 없을 때 종료한다. 

inertia value: 군집화가 된 후에 각 중심점에서 군집에 속해있는 데이터간의 거리의 합, 군집의 응집도
값이 작을 수록 응집도가 높고 군집화가 잘되었다고 평가함.
